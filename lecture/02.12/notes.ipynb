{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 141B Lecture 11\n",
    "\n",
    "The class website is <https://github.com/2019-winter-ucdavis-sta141b/notes>\n",
    "\n",
    "### Announcements\n",
    "\n",
    "\n",
    "### Topics\n",
    "\n",
    "* Web Scraping\n",
    "* Text Mining\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* [Craigslist Apartments](https://sacramento.craigslist.org/d/apts-housing-for-rent/search/apa)\n",
    "\n",
    "### References\n",
    "\n",
    "+ Web Scraping\n",
    "    * [MDN HTML Reference](https://developer.mozilla.org/en-US/docs/Web/HTML/Element)\n",
    "    * [XPath Diner](http://www.topswagcode.com/xpath/) -- an interactive XPath tutorial\n",
    "    * [CSS Diner](https://flukeout.github.io/) -- an interactive CSS Selector tutorial\n",
    "+ Natural Language Processing\n",
    "    * [Natural Language Processing with Python][nlpp], chapters 1-3. Beware: the print version is for Python 2.\n",
    "    * [Applied Text Analysis with Python][atap], chapters 1, 3.\n",
    "\n",
    "[PDSH]: https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "[ProGit]: https://git-scm.com/book/\n",
    "[nlpp]: https://www.nltk.org/book/\n",
    "[atap]: https://search.library.ucdavis.edu/primo-explore/fulldisplay?docid=01UCD_ALMA51320822340003126&context=L&vid=01UCD_V1&search_scope=everything_scope&tab=default_tab&lang=en_US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our usual data science tools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp # other science tools\n",
    "# statsmodels -- \"traditional\" statistical models\n",
    "# scikit-learn -- machine learning models\n",
    "import seaborn as sns\n",
    "#from plotnine import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Web scraping tools\n",
    "import lxml.html as lx\n",
    "import requests\n",
    "import requests_cache\n",
    "\n",
    "requests_cache.install_cache(\"../craigslist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Craigslist Apartments\n",
    "\n",
    "[Craigslist](https://www.craigslist.org/) is a popular website where people can post advertisements for free. We can use data from Craigslist to analyze the local rental market for apartments.\n",
    "\n",
    "Craigslist doesn't provide an API, so we have to scrape the data ourselves. Scraping Craigslist is the biggest challenge we've faced yet, since each ad is on a separate page.\n",
    "\n",
    "We can start by scraping the front page of the [apartments section](https://sacramento.craigslist.org/d/apts-housing-for-rent/search/apa) for links to individual ads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = \"https://sacramento.craigslist.org/d/apts-housing-for-rent/search/apa\"\n",
    "\n",
    "def scrape_front_page(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    html = lx.fromstring(response.text)\n",
    "    html.make_links_absolute(url)\n",
    "\n",
    "    html\n",
    "\n",
    "    # Get all <a> tags with class \"result-title\"\n",
    "    links = html.xpath(\"//a[contains(@class, 'result-title')]/@href\")\n",
    "    \n",
    "    next_page = html.xpath(\"//a[contains(@class, 'next')]/@href\")[0]\n",
    "    \n",
    "    return next_page, links\n",
    "\n",
    "next_page, links = scrape_front_page(start_url)\n",
    "#scrape_front_page(next_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = links[0]\n",
    "link\n",
    "\n",
    "response = requests.get(link)\n",
    "try:\n",
    "    response.raise_for_status()\n",
    "except:\n",
    "    print(\"The url couldn't be downloaded!\")\n",
    "\n",
    "html = lx.fromstring(response.text)\n",
    "\n",
    "price = html.xpath(\"//*[contains(@class, 'price')]\")[0]\n",
    "price\n",
    "\n",
    "# Alternative using CSS selectors:\n",
    "# html.cssselect(\".price\") \n",
    "\n",
    "title = html.cssselect(\"#titletextonly\")[0].text_content()\n",
    "\n",
    "#html.cssselect(\"p.attrgroup span\")\n",
    "[x.text_content() for x in html.xpath(\"//p[contains(@class, 'attrgroup')]/span\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "A _natural language_ is a language people use to communicate, like English, Spanish, or Mandarin. These languages evolved over thousands of years and do not have simple, explicit rules.\n",
    "\n",
    "_Natural language processing_ (NLP) means using a computer to analyze, manipulate, or synthesize natural language. Some examples of NLP tasks are:\n",
    "* Translating from one language to another\n",
    "* Recognizing speech or handwriting\n",
    "* Tagging sentences with metadata, such as parts of speech (verbs, nouns, etc) or sentiment\n",
    "* Extracting information or computing statistics from text\n",
    "\n",
    "Compared to artificial languages like Python and XML, it's much more difficult to extract information from natural languages. NLP is a wide field; we only have time to learn the absolute basics. If you want to learn more, consider reading the entire [Natural Language Processing with Python][nlpp] book or taking a class in computational linguistics.\n",
    "\n",
    "[nlpp]: https://www.nltk.org/book/\n",
    "\n",
    "\n",
    "### The Python NLP Ecosystem\n",
    "\n",
    "There are lots of Python packages for NLP (try searching online)! A few popular ones are:\n",
    "\n",
    "* [Natural Language Tool Kit][nltk] (__nltk__) is the most popular. It's designed for learning and research, so it's well-documented and has lots of features.\n",
    "* [TextBlob][textblob] is a \"simplified\" package. It has a nicer interface than NLTK, but less features.\n",
    "* [SpaCy][spacy] is a \"production-ready\" package, and the fastest of all the packages listed here. Useful for working with large natural language datasets.\n",
    "* [gensim][gensim] is a package for creating topic models, which are a kind of statistical model that predict the topics of a text.\n",
    "\n",
    "We're going to learn __nltk__, but you might want to try some of the others if your project involves NLP.\n",
    "\n",
    "[Stanford's Core NLP][CoreNLP] library is at the cutting edge of NLP research. It's developed in Java, but several Python packages provide an interface (such as [pynlp][] and [stanford-corenlp][]).\n",
    "\n",
    "[nltk]: https://www.nltk.org/\n",
    "[spacy]: https://spacy.io/\n",
    "[textblob]: https://textblob.readthedocs.io/en/dev/\n",
    "[gensim]: https://radimrehurek.com/gensim/\n",
    "[CoreNLP]: https://stanfordnlp.github.io/CoreNLP/\n",
    "[pynlp]: https://github.com/sina-al/pynlp\n",
    "[stanford-corenlp]: https://github.com/Lynten/stanford-corenlp\n",
    "\n",
    "### Installing NLTK\n",
    "\n",
    "In an Anaconda Prompt (Win) or Terminal (MacOS & Linux), run:\n",
    "\n",
    "```shell\n",
    "conda install -c anaconda nltk\n",
    "```\n",
    "\n",
    "Then try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora and Documents\n",
    "\n",
    "A _document_ is a single body text. When working with natural language data, documents are the unit of observation.\n",
    "\n",
    "What you choose as a document depends on the purpose of your analysis. If you're studying how people react to news on Twitter, it makes sense to use individual tweets as documents. If you're studying how animals are portrayed in 19th-century literature, you could use individual novels as documents.\n",
    "\n",
    "A _corpus_ is a collection of documents. In other words, a corpus is a dataset.\n",
    "\n",
    "__nltk__ provides some example corpora in the `nltk.corpus` submodule. The documentation gives a [complete list](http://www.nltk.org/nltk_data/). Most have to be downloaded with `nltk.download()` before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus\n",
    "\n",
    "# Download books from Project Gutenberg\n",
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.fileids()` method lists the documents in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.raw()` method returns the raw text for a single document. Specify the document by its file ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "A _token_ is a sequence of characters to be treated as a group. Tokens are the unit of analysis for an indvidual document.\n",
    "\n",
    "Tokens can represent paragraphs, sentences, words, or something else. Most of the time, tokens will be words.\n",
    "\n",
    "When you analyze a document, the first step will usually be to split the document into tokens. Functions that do this are called _tokenizers_, and this process is called _tokenization_.\n",
    "\n",
    "The `nltk.sent_tokenize()` function splits a document into sentences, and the `nltk.word_tokenize()` function splits a document into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpora also have `.sents()` and `.word()` methods for tokenization. These methods are specialized to the corpus, so they sometimes use the different strategies than `sent_tokenize()` and `word_tokenize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings, String Methods, and Regular Expressions\n",
    "\n",
    "How does word tokenization actually work?\n",
    "\n",
    "The simplest strategy is to split at whitespace. You can do this with Python's built-in string methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting on whitespace doesn't handle punctuation. You can use regular expressions to split on more complex patterns. Python's built-in __re__ module provides regular expression functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we also want to split at newlines?\n",
    "\n",
    "#### Escape Sequences and Raw Strings\n",
    "\n",
    "In Python strings, backslash `\\` marks the beginning of an _escape sequence_. Escape sequences are special codes for writing characters that you can't otherwise type. For example, `\\n` is a new line character and `\\t` is a tab character.\n",
    "\n",
    "Since `\\` has a special meaning in strings, to write a literal `\\` you must use the escape sequence `\\\\`.\n",
    "\n",
    "You can see the actual characters in a string by printing the string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expression language is independent of Python and also uses backslash `\\` to mark the beginning of an escape sequence. Regex escape sequences disable special behavior for characters. For example, `.` matches any character, but `\\.` only matches a literal `.`.\n",
    "\n",
    "As a result, writing a regular expression in an ordinary Python string is awkward. For example, to match a literal `\\`, we need to write `\\\\` in regular expressions, which is `\\\\\\\\` in an ordinary Python string.\n",
    "\n",
    "Python provides _raw strings_, where `\\` has no special meaning for Python, to help solve this problem. You can create a raw string by putting an `r` before the starting quote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even raw strings can't end in `\\`; this is a limitation of the Python parser.\n",
    "\n",
    "#### More Regular Expressions\n",
    "\n",
    "Now we can write a better regular expression to split with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expressions language includes _character classes_ that describe common sets of characters. The whitespace class `\\s` and the word class `\\w` are useful here. So to split on any whitespace character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capitalizing a character classes inverts the meaning, so to split on all non-word characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than splitting the text, you can also approach the problem from the perspective of extracting tokens. The `findall()` function returns all matches for a regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing natural languages is a difficult problem. Some tokenizers work better for certain kinds of documents than others.\n",
    "\n",
    "Before building your own tokenizer, try the tokenizers included with __nltk__, in the `nltk.tokenize` submodule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing Text\n",
    "\n",
    "We standardize numerical data in order to make fair comparisons, comparisons that are not influenced by the location and scale of the data. Similarly, you can standardize text (tokens) to make sure comparisons are fair and accurate.\n",
    "\n",
    "For example, `\"Cat\"` and `\"cat\"` are the same word even though they're different tokens. Converting all characters to lowercase is one way to standardize a document.\n",
    "\n",
    "Some common standardization techniques for text are:\n",
    "\n",
    "* Lowercasing\n",
    "* Stemming: Use patterns to remove prefixes and suffixes from words.\n",
    "* Lemmatiziation: Look up each token in a dictionary and replace it with a root word. Similar to stemming, but more accurate.\n",
    "* Stopword Removal: Remove tokens that don't contribute meaning. For example, \"the\" is meaningless on its own.\n",
    "* Identifying Outliers: Identify and possibly remove non-standard \"words\" like numbers, mispellings, code, etc...\n",
    "\n",
    "How and whether you should standardize a document or corpus depends on what kind of analysis you want to do. There is no formula; you must think carefully and experiment to determine which standardization techniques work best for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercasing\n",
    "\n",
    "You can use Python's string methods for simple text transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "_Stemming_ runs an algorithm on each token to remove affixes (prefixes and suffixes). The result is called a _stem_.\n",
    "\n",
    "Stemming is useful if you want to ignore affixes.\n",
    "\n",
    "For example, most English verbs use suffixes to mark the tense. We write \"They fish\" (present) and \"They fished\" (past). Without any standardization, the tokens \"fish\" and \"fished\" would be treated as separate words. Stemming converts both tokens to the common stem \"fish\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to stem an entire document, use a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmers use a sequence of rules to determine the stem for each token, but natural languages are full of special cases and exceptions. So as you can see in the example above, some stems are not words (\"alic\"), and sometimes tokens that seem like they should have the same stem don't.\n",
    "\n",
    "Several different stemmers are provided in the `nltk.stem` submodule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "_Lemmatization_ looks up each token in a dictionary to find a root word, or _lemma_.\n",
    "\n",
    "Lemmatization serves the same purpose as stemming. Lemmatization is more accurate, but requires a dictionary and usually takes longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WordNet lemmatizer requires part of speech information in order to lemmatize words. You can get approximate part of speech information with __nltk__'s `pos_tag()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are [Brown POS tags][brown], but the lemmatizer uses WordNet POS tags. You can use this function to convert the tags:\n",
    "\n",
    "[brown]: https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def wordnet_pos(tag):\n",
    "    \"\"\"Map a Brown POS tag to a WordNet POS tag.\"\"\"\n",
    "    \n",
    "    table = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
    "    \n",
    "    # Default to a noun.\n",
    "    return table.get(tag[0], wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk.stem` submodule also provides several different lemmatizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword Removal\n",
    "\n",
    "_Stopwords_ are words that appear frequently but don't add meaning.\n",
    "\n",
    "In English, \"the\", \"a\", and \"at\" are examples. However, exactly which words are stopwords depends on your analysis. Words that are meaningless in one analysis might be very important in others.\n",
    "\n",
    "You can filter out stopwords with a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__nltk__ also provides a stopwords corpus that contains common stopwords for several languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Documents\n",
    "\n",
    "A simple way to explore a document is by looking at frequency distributions for tokens.\n",
    "\n",
    "You can use the `FreqDist()` function to construct a frequency distributions from a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency distribution objects have a few methods to provide summary information.\n",
    "\n",
    "The `.most_common()` method returns the most common tokens and their frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _hapax_ is a token that only occurs once within a document. The `.hapaxes()` method returns the hapaxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.plot()` method displays a plot of word frequencies, sorted from most to least frequent word.\n",
    "\n",
    "The first parameter controls how many words to display. The second parameter controls whether the plot is cummulative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
