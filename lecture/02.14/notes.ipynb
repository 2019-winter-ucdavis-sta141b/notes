{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 141B Lecture 12\n",
    "\n",
    "The class website is <https://github.com/2019-winter-ucdavis-sta141b/notes>\n",
    "\n",
    "### Announcements\n",
    "\n",
    "### Topics\n",
    "\n",
    "* Natural Language Processing\n",
    "\n",
    "### Datasets\n",
    "\n",
    "### References\n",
    "\n",
    "* [Natural Language Processing with Python][nlpp], chapters 1-3. Beware: the print version is for Python 2.\n",
    "* [Applied Text Analysis with Python][atap], chapters 1, 3, 4\n",
    "* [Scikit-Learn Documentation][skl], especially the section about [Text Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "[PDSH]: https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "[ProGit]: https://git-scm.com/book/\n",
    "[nlpp]: https://www.nltk.org/book/\n",
    "[atap]: https://search.library.ucdavis.edu/primo-explore/fulldisplay?docid=01UCD_ALMA51320822340003126&context=L&vid=01UCD_V1&search_scope=everything_scope&tab=default_tab&lang=en_US\n",
    "[skl]: https://scikit-learn.org/stable/documentation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "\n",
    "# nltk.download(\"gutenberg\")\n",
    "# nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = nltk.corpus.gutenberg.raw(\"carroll-alice.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings, String Methods, and Regular Expressions\n",
    "\n",
    "How does word tokenization actually work?\n",
    "\n",
    "The simplest strategy is to split at whitespace. You can do this with Python's built-in string methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting on whitespace doesn't handle punctuation. You can use regular expressions to split on more complex patterns. Python's built-in __re__ module provides regular expression functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "re.split(\"[ ,.:;!']\", alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we also want to split at newlines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escape Sequences and Raw Strings\n",
    "\n",
    "In Python strings, backslash `\\` marks the beginning of an _escape sequence_. Escape sequences are special codes for writing characters that you can't otherwise type. For example, `\\n` is a new line character and `\\t` is a tab character.\n",
    "\n",
    "Since `\\` has a special meaning in strings, to write a literal `\\` you must use the escape sequence `\\\\`.\n",
    "\n",
    "You can see the actual characters in a string by printing the string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expression language is independent of Python and also uses backslash `\\` to mark the beginning of an escape sequence. Regex escape sequences disable special behavior for characters. For example, `.` matches any character, but `\\.` only matches a literal `.`.\n",
    "\n",
    "As a result, writing a regular expression in an ordinary Python string is awkward. For example, to match a literal `\\`, we need to write `\\\\` in regular expressions, which is `\\\\\\\\` in an ordinary Python string.\n",
    "\n",
    "Python provides _raw strings_, where `\\` has no special meaning for Python, to help solve this problem. You can create a raw string by putting an `r` before the starting quote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even raw strings can't end in `\\`; this is a limitation of the Python parser.\n",
    "\n",
    "### More Regular Expressions\n",
    "\n",
    "Now we can write a better regular expression to split with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expressions language includes _character classes_ that describe common sets of characters. The whitespace class `\\s` and the word class `\\w` are useful here. So to split on any whitespace character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capitalizing a character classes inverts the meaning, so to split on all non-word characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than splitting the text, you can also approach the problem from the perspective of extracting tokens. The `findall()` function returns all matches for a regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing natural languages is a difficult problem. Some tokenizers work better for certain kinds of documents than others.\n",
    "\n",
    "Before building your own tokenizer, try the tokenizers included with __nltk__, in the `nltk.tokenize` submodule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing Text\n",
    "\n",
    "We standardize numerical data in order to make fair comparisons, comparisons that are not influenced by the location and scale of the data. Similarly, you can standardize text (tokens) to make sure comparisons are fair and accurate.\n",
    "\n",
    "For example, `\"Cat\"` and `\"cat\"` are the same word even though they're different tokens. Converting all characters to lowercase is one way to standardize a document.\n",
    "\n",
    "Some common standardization techniques for text are:\n",
    "\n",
    "* Lowercasing\n",
    "* Stemming: Use patterns to remove prefixes and suffixes from words.\n",
    "* Lemmatiziation: Look up each token in a dictionary and replace it with a root word. Similar to stemming, but more accurate.\n",
    "* Stopword Removal: Remove tokens that don't contribute meaning. For example, \"the\" is meaningless on its own.\n",
    "* Identifying Outliers: Identify and possibly remove non-standard \"words\" like numbers, mispellings, code, etc...\n",
    "\n",
    "How and whether you should standardize a document or corpus depends on what kind of analysis you want to do. There is no formula; you must think carefully and experiment to determine which standardization techniques work best for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing\n",
    "\n",
    "You can use Python's string methods for simple text transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "_Stemming_ runs an algorithm on each token to remove affixes (prefixes and suffixes). The result is called a _stem_.\n",
    "\n",
    "Stemming is useful if you want to ignore affixes.\n",
    "\n",
    "For example, most English verbs use suffixes to mark the tense. We write \"They fish\" (present) and \"They fished\" (past). Without any standardization, the tokens \"fish\" and \"fished\" would be treated as separate words. Stemming converts both tokens to the common stem \"fish\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to stem an entire document, use a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmers use a sequence of rules to determine the stem for each token, but natural languages are full of special cases and exceptions. So as you can see in the example above, some stems are not words (\"alic\"), and sometimes tokens that seem like they should have the same stem don't.\n",
    "\n",
    "Several different stemmers are provided in the `nltk.stem` submodule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "_Lemmatization_ looks up each token in a dictionary to find a root word, or _lemma_.\n",
    "\n",
    "Lemmatization serves the same purpose as stemming. Lemmatization is more accurate, but requires a dictionary and usually takes longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WordNet lemmatizer requires part of speech information in order to lemmatize words. You can get approximate part of speech information with __nltk__'s `pos_tag()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are [Brown POS tags][brown], but the lemmatizer uses WordNet POS tags. You can use this function to convert the tags:\n",
    "\n",
    "[brown]: https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def wordnet_pos(tag):\n",
    "    \"\"\"Map a Brown POS tag to a WordNet POS tag.\"\"\"\n",
    "    \n",
    "    table = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
    "    \n",
    "    # Default to a noun.\n",
    "    return table.get(tag[0], wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk.stem` submodule also provides several different lemmatizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "_Stopwords_ are words that appear frequently but don't add meaning.\n",
    "\n",
    "In English, \"the\", \"a\", and \"at\" are examples. However, exactly which words are stopwords depends on your analysis. Words that are meaningless in one analysis might be very important in others.\n",
    "\n",
    "You can filter out stopwords with a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__nltk__ also provides a stopwords corpus that contains common stopwords for several languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Documents\n",
    "\n",
    "A simple way to explore a document is by looking at frequency distributions for tokens.\n",
    "\n",
    "You can use the `FreqDist()` function to construct a frequency distributions from a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency distribution objects have a few methods to provide summary information.\n",
    "\n",
    "The `.most_common()` method returns the most common tokens and their frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _hapax_ is a token that only occurs once within a document. The `.hapaxes()` method returns the hapaxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.plot()` method displays a plot of word frequencies, sorted from most to least frequent word.\n",
    "\n",
    "The first parameter controls how many words to display. The second parameter controls whether the plot is cummulative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Most statistical techniques take numbers as input. You may have already noticed this when working with categorical data. We can't compute the mean, median, standard deviation, or z-score if the observations aren't numbers. While we can fit linear models, it takes extra work because we have to create, or _engineer_, indicator variables.\n",
    "\n",
    "We face the same problem with natural language data. We need to _quantify_ documents, or turn them into numbers, so that we can use a wider variety of statistical techniques. We can do this by engineering features from our documents.\n",
    "\n",
    "So: what kinds of features can we create for language data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequencies\n",
    "\n",
    "One solution is to extend the idea of frequency analysis. We used frequency analysis to study individual documents, but what if we compute the word frequencies for every document in our corpus, and use those frequencies as features?\n",
    "\n",
    "Let's try this for a small corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"The cat saw the dog was angry.\", \"The dog saw the cat was angry.\", \"The canary saw the iguana was sad.\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we use term frequencies as features, we lose information about the order of the words in each document.\n",
    "\n",
    "The first and second document contain the same words, but in different orders. The word frequency features for these two documents are identical.\n",
    "\n",
    "The __scikit-learn__ package (included with Anaconda) provides functions to help with feature engineering. The `sklearn.feature_extraction.text` submodule is specifically for extracting features from text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with term frequencies is that some terms have high frequencies simply because they appear frequently in the language. These terms can cause documents to appear similar even if they are otherwise different.\n",
    "\n",
    "While removing stopwords takes care of some high-frequency words, there may also be high-frequency words that have meaning and need to be kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding\n",
    "\n",
    "We can avoid emphasis on high-frequency words by ignoring frequency altogether. Instead, we can create indicator variables for individual words. The indicator is 1 if the word appears in the document, and 0 otherwise.\n",
    "\n",
    "In machine learning, an indicator variable is also called a _one-hot encoding_.\n",
    "\n",
    "The `sklearn.preprocessing` submodule of __scikit-learn__ provides a function for one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with term frequencies, we lose information about the order of the words in the document.\n",
    "\n",
    "One-hot encoding as an extreme transformation: every term is equally important. This means terms that are relatively rare or unique still might be underemphasized (this is also a problem for term frequencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Term frequency-inverse document frequency_ (tf-idf) statistics put terms on approximately the same scale while also emphasizing relatively rare terms. There are [several different tf-idf statistics](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "The _smoothed tf-idf_, for a term $t$ and document $d$, is given by:\n",
    "\n",
    "$$\n",
    "\\operatorname{tf-idf}(t, d) = \\operatorname{tf}(t, d) \\cdot \\log \\left( \\frac{N}{1 + n_t} \\right)\n",
    "$$\n",
    "\n",
    "where $N$ is the total number of documents and $n_t$ is the number of documents that contain $t$.\n",
    "\n",
    "The `sklearn.feature_extraction.text` submodule of __scikit-learn__ provides a function for computing tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In long documents or documents with many high-frequency terms, we can further reduce the emphasis on these terms by taking the logarithm of the term frequency. To do this, set `sublinear_tf = True` in the `TfidfVectorizer()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bag-of-words Model\n",
    "\n",
    "The one-hot encoding, term frequencies, and TF-IDF scores all ignore word order.\n",
    "\n",
    "The _bag-of-words model_ assumes that the order of words in a document doesn't matter. Imagine taking the words in each document and dumping them into a bag, where they get all mixed up. Note that in this case \"model\" means a way of thinking about a problem, not a statistical model.\n",
    "\n",
    "While the order of words in a document might seem important, the bag-of-words model is surprisingly useful. The bag-of-words model is a good place to start if you want to use statistical methods on language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Similarity\n",
    "\n",
    "We can measure the _similarity_ of two documents by computing the distance between their term frequency vectors. There are many different ways we can measure distance:\n",
    "\n",
    "* Minkowski distance, a family of distances that includes Euclidean distance and Manhattan distance.\n",
    "* Mahalanobis distance, the Euclidean distance between z-scores.\n",
    "* Cosine distance, the cosine of the angle between two vectors. See [here](https://stats.stackexchange.com/a/235676/29695) for an explanation of how cosine distance is related to correlation.\n",
    "* And others...\n",
    "\n",
    "The cosine distance often works well for language data. The cosine distance between two vectors $a$ and $b$ is defined as:\n",
    "\n",
    "$$\n",
    "\\frac{a \\cdot b}{\\Vert a \\Vert \\Vert b \\Vert}\n",
    "$$\n",
    "\n",
    "where $\\Vert \\cdot \\Vert$ is the Euclidean norm.\n",
    "\n",
    "The `TfidfVectorizer()` function already divides the returned tf-idf vectors by their Euclidean norms, so we can compute cosine distance as a simple dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The n-gram Model\n",
    "\n",
    "Remember how the bag-of-words model ignores word order?\n",
    "\n",
    "We can extend the model to keep some order by taking sequences of words instead of individual words. Sequences of two or three words are called _bigrams_ and _trigrams_, respectively. A sequence of $n$ words is called an _n-gram_.\n",
    "\n",
    "__nltk__ provides functions to extract n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a separate n-gram was computed for each word in the original document. So for the bigrams in the example, we get every pair of words that appears in the document.\n",
    "\n",
    "The n-gram model assumes that nearby words have the strongest effect on the meaning of each word.\n",
    "\n",
    "We can use n-grams to identify phrases that are particularly common in a document. We can also use the n-gram model to engineer features, the same way we used the bag-of-words model. That is, we can compute frequencies, one-hot encodings, TF-IDFs, and other features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
