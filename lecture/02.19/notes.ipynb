{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 141B Lecture 13\n",
    "\n",
    "The class website is <https://github.com/2019-winter-ucdavis-sta141b/notes>\n",
    "\n",
    "### Announcements\n",
    "\n",
    "* Assignment 3 grades will be posted later today\n",
    "\n",
    "### Topics\n",
    "\n",
    "* Natural Language Processing (wrap-up)\n",
    "\n",
    "### Datasets\n",
    "\n",
    "### References\n",
    "\n",
    "* [Natural Language Processing with Python][nlpp], chapters 1-3. Beware: the print version is for Python 2.\n",
    "* [Applied Text Analysis with Python][atap], chapters 1, 3, 4\n",
    "\n",
    "[PDSH]: https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "[ProGit]: https://git-scm.com/book/\n",
    "[nlpp]: https://www.nltk.org/book/\n",
    "[atap]: https://search.library.ucdavis.edu/primo-explore/fulldisplay?docid=01UCD_ALMA51320822340003126&context=L&vid=01UCD_V1&search_scope=everything_scope&tab=default_tab&lang=en_US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "\n",
    "# nltk.download(\"gutenberg\")\n",
    "# nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for Natural Language Data\n",
    "\n",
    "Most statistical techniques take numbers as input. You may have already noticed this when working with categorical data. We can't compute the mean, median, standard deviation, or z-score if the observations aren't numbers. While we can fit linear models, it takes extra work because we have to create, or _engineer_, indicator variables.\n",
    "\n",
    "We face the same problem with natural language data. We need to _quantify_ documents, or turn them into numbers, so that we can use a wider variety of statistical techniques. We can do this by engineering features from our documents.\n",
    "\n",
    "So: what kinds of features can we create for language data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequencies\n",
    "\n",
    "One solution is to extend the idea of frequency analysis. We used frequency analysis to study individual documents, but what if we compute the word frequencies for every document in our corpus, and use those frequencies as features?\n",
    "\n",
    "Let's try this for a small corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angry</th>\n",
       "      <th>canary</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>iguana</th>\n",
       "      <th>sad</th>\n",
       "      <th>saw</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   angry  canary  cat  dog  iguana  sad  saw  was\n",
       "0    1.0     0.0  1.0  1.0     0.0  0.0    1    1\n",
       "1    1.0     0.0  1.0  1.0     0.0  0.0    1    1\n",
       "2    0.0     1.0  0.0  0.0     1.0  1.0    1    1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"The cat saw the dog was angry.\", \"The dog saw the cat was angry.\", \"The canary saw the iguana was sad.\"]\n",
    "\n",
    "#doc = corpus[0]\n",
    "def get_freq_dist(doc):\n",
    "    words = (w.lower() for w in nltk.word_tokenize(doc))\n",
    "    words = (w for w in words if w not in [\"the\", \"a\"] and w.isalnum())\n",
    "    return nltk.FreqDist(words)\n",
    "\n",
    "df = pd.DataFrame([get_freq_dist(doc) for doc in corpus])\n",
    "df = df.fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\".\".isalnum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we use term frequencies as features, we lose information about the order of the words in each document.\n",
    "\n",
    "The first and second document contain the same words, but in different orders. The word frequency features for these two documents are identical.\n",
    "\n",
    "The __scikit-learn__ package (included with Anaconda) provides functions to help with feature engineering. The `sklearn.feature_extraction.text` submodule is specifically for extracting features from text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 0, 1, 1, 0, 0, 1, 2, 1],\n",
       "        [1, 1, 0, 1, 1, 0, 0, 1, 2, 1],\n",
       "        [1, 0, 1, 0, 0, 1, 1, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(tokenizer = nltk.word_tokenize)\n",
    "freq = vec.fit_transform(corpus)\n",
    "\n",
    "# Convert sparse matrix to dense matrix\n",
    "# Don't do this for a really large matrix\n",
    "freq.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #F00; font-weight: bold\">NOTE</span>\n",
    "\n",
    "_In lecture I couldn't remember the name of the method for this, but here it is now. -Nick_\n",
    "\n",
    "Use the `.get_feature_names()` method to see which term each column corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'angry', 'canary', 'cat', 'dog', 'iguana', 'sad', 'saw', 'the', 'was']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function word_tokenize at 0x7efcf08d86a8>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with term frequencies is that some terms have high frequencies simply because they appear frequently in the language. These terms can cause documents to appear similar even if they are otherwise different.\n",
    "\n",
    "While removing stopwords takes care of some high-frequency words, there may also be high-frequency words that have meaning and need to be kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding\n",
    "\n",
    "We can avoid emphasis on high-frequency words by ignoring frequency altogether. Instead, we can create indicator variables for individual words. The indicator is 1 if the word appears in the document, and 0 otherwise.\n",
    "\n",
    "In machine learning, an indicator variable is also called a _one-hot encoding_.\n",
    "\n",
    "The `sklearn.preprocessing` submodule of __scikit-learn__ provides a function for one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 0, 1, 1, 0, 0, 1, 1, 1],\n",
       "        [1, 1, 0, 1, 1, 0, 0, 1, 1, 1],\n",
       "        [1, 0, 1, 0, 0, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(freq > 0).todense()\n",
    "\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "binarizer = Binarizer()\n",
    "ohot = binarizer.fit_transform(freq)\n",
    "\n",
    "\n",
    "ohot.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with term frequencies, we lose information about the order of the words in the document.\n",
    "\n",
    "One-hot encoding as an extreme transformation: every term is equally important. This means terms that are relatively rare or unique still might be underemphasized (this is also a problem for term frequencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Term frequency-inverse document frequency_ (tf-idf) statistics put terms on approximately the same scale while also emphasizing relatively rare terms. There are [several different tf-idf statistics](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "The _smoothed tf-idf_, for a term $t$ and document $d$, is given by:\n",
    "\n",
    "$$\n",
    "\\operatorname{tf-idf}(t, d) = \\operatorname{tf}(t, d) \\cdot \\log \\left( \\frac{N}{1 + n_t} \\right)\n",
    "$$\n",
    "\n",
    "where $N$ is the total number of documents and $n_t$ is the number of documents that contain $t$.\n",
    "\n",
    "The `sklearn.feature_extraction.text` submodule of __scikit-learn__ provides a function for computing tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.30371264, 0.39108532, 0.        , 0.39108532, 0.39108532,\n",
       "         0.        , 0.        , 0.30371264, 0.5142302 , 0.30371264],\n",
       "        [0.30371264, 0.39108532, 0.        , 0.39108532, 0.39108532,\n",
       "         0.        , 0.        , 0.30371264, 0.5142302 , 0.30371264],\n",
       "        [0.26291231, 0.        , 0.44514923, 0.        , 0.        ,\n",
       "         0.44514923, 0.44514923, 0.26291231, 0.44514923, 0.26291231]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(tokenizer = nltk.word_tokenize, sublinear_tf = True)\n",
    "tfidf = vec.fit_transform(corpus)\n",
    "\n",
    "tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In long documents or documents with many high-frequency terms, we can further reduce the emphasis on these terms by taking the logarithm of the term frequency. To do this, set `sublinear_tf = True` in the `TfidfVectorizer()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bag-of-words Model\n",
    "\n",
    "The one-hot encoding, term frequencies, and TF-IDF scores all ignore word order.\n",
    "\n",
    "The _bag-of-words model_ assumes that the order of words in a document doesn't matter. Imagine taking the words in each document and dumping them into a bag, where they get all mixed up. Note that in this case \"model\" means a way of thinking about a problem, not a statistical model.\n",
    "\n",
    "While the order of words in a document might seem important, the bag-of-words model is surprisingly useful. The bag-of-words model is a good place to start if you want to use statistical methods on language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Similarity\n",
    "\n",
    "We can measure the _similarity_ of two documents by computing the distance between their term frequency vectors. There are many different ways we can measure distance and similarity:\n",
    "\n",
    "* Minkowski distance, a family of distances that includes Euclidean distance and Manhattan distance.\n",
    "* Mahalanobis distance, the Euclidean distance between z-scores.\n",
    "* Cosine similarity, the cosine of the angle between two vectors. See [here](https://stats.stackexchange.com/a/235676/29695) for an explanation of how cosine similarity is related to correlation. Note that the range of cosine is $[-1, 1]$ and $\\cos(0) = 1$, so vectors that are close together will have a cosine similarity close to 1, not 0.\n",
    "* And others...\n",
    "\n",
    "Cosine similarity often works well for language data. The cosine similarity between two vectors $a$ and $b$ is defined as:\n",
    "\n",
    "$$\n",
    "\\frac{a \\cdot b}{\\Vert a \\Vert \\Vert b \\Vert}\n",
    "$$\n",
    "\n",
    "where $\\Vert \\cdot \\Vert$ is the Euclidean norm.\n",
    "\n",
    "The `TfidfVectorizer()` function already divides the returned tf-idf vectors by their Euclidean norms, so we can compute cosine similarity as a simple dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1.        , 1.        , 0.46845855],\n",
       "        [1.        , 1.        , 0.46845855],\n",
       "        [0.46845855, 0.46845855, 1.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tfidf @ tfidf.T).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #F00; font-weight: bold\">NOTE</span>\n",
    "\n",
    "_Here's an answer to a question I didn't have a good answer for in lecture. -Nick_\n",
    "\n",
    "Part of the reason that cosine similarity is a good measure in NLP is that cosine similarity, like correlation, is not affected by the scale of the vector elements. For vectors that contain term frequencies (or functions of term frequencies), this means that the length of the original documents will not affect whether or not they are similar -- only their word content will."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The n-gram Model\n",
    "\n",
    "Remember how the bag-of-words model ignores word order?\n",
    "\n",
    "We can extend the model to keep some order by taking sequences of words instead of individual words. Sequences of two or three words are called _bigrams_ and _trigrams_, respectively. A sequence of $n$ words is called an _n-gram_.\n",
    "\n",
    "__nltk__ provides functions to extract n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'cat', 'saw'),\n",
       " ('cat', 'saw', 'the'),\n",
       " ('saw', 'the', 'dog'),\n",
       " ('the', 'dog', 'was'),\n",
       " ('dog', 'was', 'angry'),\n",
       " ('was', 'angry', '.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [nltk.word_tokenize(doc) for doc in corpus]\n",
    "list(nltk.ngrams(words[0], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'dog', 'saw'),\n",
       " ('dog', 'saw', 'the'),\n",
       " ('saw', 'the', 'cat'),\n",
       " ('the', 'cat', 'was'),\n",
       " ('cat', 'was', 'angry'),\n",
       " ('was', 'angry', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(words[1], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a separate n-gram was computed for each word in the original document. So for the bigrams in the example, we get every pair of words that appears in the document.\n",
    "\n",
    "The n-gram model assumes that nearby words have the strongest effect on the meaning of each word.\n",
    "\n",
    "We can use n-grams to identify phrases that are particularly common in a document. We can also use the n-gram model to engineer features, the same way we used the bag-of-words model. That is, we can compute frequencies, one-hot encodings, TF-IDFs, and other features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.35221512, 0.        , 0.46312056, 0.        , 0.        ,\n",
       "         0.46312056, 0.        , 0.        , 0.27352646, 0.        ,\n",
       "         0.35221512, 0.35221512, 0.        , 0.35221512, 0.        ],\n",
       "        [0.35221512, 0.        , 0.        , 0.46312056, 0.46312056,\n",
       "         0.        , 0.        , 0.        , 0.27352646, 0.        ,\n",
       "         0.35221512, 0.35221512, 0.        , 0.35221512, 0.        ],\n",
       "        [0.        , 0.39687454, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.39687454, 0.39687454, 0.2344005 , 0.39687454,\n",
       "         0.        , 0.        , 0.39687454, 0.        , 0.39687454]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Word-tokenize data ahead of time since we'll need tokens to compute bigrams\n",
    "words = [nltk.word_tokenize(d.lower()) for d in corpus]\n",
    "\n",
    "# nltk.bigrams() is a function for computing bigrams, equivalent to nltk.ngrams(DATA, 2)\n",
    "vectorizer = TfidfVectorizer(tokenizer = nltk.bigrams, lowercase = False)\n",
    "tfidf = vectorizer.fit_transform(words)\n",
    "\n",
    "tfidf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('angry', '.'),\n",
       " ('canary', 'saw'),\n",
       " ('cat', 'saw'),\n",
       " ('cat', 'was'),\n",
       " ('dog', 'saw'),\n",
       " ('dog', 'was'),\n",
       " ('iguana', 'was'),\n",
       " ('sad', '.'),\n",
       " ('saw', 'the'),\n",
       " ('the', 'canary'),\n",
       " ('the', 'cat'),\n",
       " ('the', 'dog'),\n",
       " ('the', 'iguana'),\n",
       " ('was', 'angry'),\n",
       " ('was', 'sad')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now each column corresponds to one bigram.\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1.        , 0.5710387 , 0.06411474],\n",
       "        [0.5710387 , 1.        , 0.06411474],\n",
       "        [0.06411474, 0.06411474, 1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As before, we can see how similar each document is to the others by examining the TF-IDF statistics.\n",
    "# By using bigrams, we can see the difference in the first and second document.\n",
    "# Note that the first and second document are still much more similar than the first and third document.\n",
    "(tfidf @ tfidf.T).todense()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
