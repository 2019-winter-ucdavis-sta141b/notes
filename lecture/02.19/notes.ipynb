{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 141B Lecture 13\n",
    "\n",
    "The class website is <https://github.com/2019-winter-ucdavis-sta141b/notes>\n",
    "\n",
    "### Announcements\n",
    "\n",
    "* Assignment 3 grades will be posted later today\n",
    "\n",
    "### Topics\n",
    "\n",
    "* Natural Language Processing (wrap-up)\n",
    "* Databases & SQL\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* The [Suppliers Database](http://nick-ulle.github.io/teach/suppliers.sqlite)\n",
    "\n",
    "### References\n",
    "\n",
    "* [Natural Language Processing with Python][nlpp], chapters 1-3. Beware: the print version is for Python 2.\n",
    "* [Applied Text Analysis with Python][atap], chapters 1, 3, 4\n",
    "* [W3 Schools SQL Tutorial](https://www.w3schools.com/sql/)\n",
    "\n",
    "[PDSH]: https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "[ProGit]: https://git-scm.com/book/\n",
    "[nlpp]: https://www.nltk.org/book/\n",
    "[atap]: https://search.library.ucdavis.edu/primo-explore/fulldisplay?docid=01UCD_ALMA51320822340003126&context=L&vid=01UCD_V1&search_scope=everything_scope&tab=default_tab&lang=en_US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "\n",
    "# nltk.download(\"gutenberg\")\n",
    "# nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = nltk.corpus.gutenberg.raw(\"carroll-alice.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for Natural Language Data\n",
    "\n",
    "Most statistical techniques take numbers as input. You may have already noticed this when working with categorical data. We can't compute the mean, median, standard deviation, or z-score if the observations aren't numbers. While we can fit linear models, it takes extra work because we have to create, or _engineer_, indicator variables.\n",
    "\n",
    "We face the same problem with natural language data. We need to _quantify_ documents, or turn them into numbers, so that we can use a wider variety of statistical techniques. We can do this by engineering features from our documents.\n",
    "\n",
    "So: what kinds of features can we create for language data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequencies\n",
    "\n",
    "One solution is to extend the idea of frequency analysis. We used frequency analysis to study individual documents, but what if we compute the word frequencies for every document in our corpus, and use those frequencies as features?\n",
    "\n",
    "Let's try this for a small corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"The cat saw the dog was angry.\", \"The dog saw the cat was angry.\", \"The canary saw the iguana was sad.\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we use term frequencies as features, we lose information about the order of the words in each document.\n",
    "\n",
    "The first and second document contain the same words, but in different orders. The word frequency features for these two documents are identical.\n",
    "\n",
    "The __scikit-learn__ package (included with Anaconda) provides functions to help with feature engineering. The `sklearn.feature_extraction.text` submodule is specifically for extracting features from text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with term frequencies is that some terms have high frequencies simply because they appear frequently in the language. These terms can cause documents to appear similar even if they are otherwise different.\n",
    "\n",
    "While removing stopwords takes care of some high-frequency words, there may also be high-frequency words that have meaning and need to be kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding\n",
    "\n",
    "We can avoid emphasis on high-frequency words by ignoring frequency altogether. Instead, we can create indicator variables for individual words. The indicator is 1 if the word appears in the document, and 0 otherwise.\n",
    "\n",
    "In machine learning, an indicator variable is also called a _one-hot encoding_.\n",
    "\n",
    "The `sklearn.preprocessing` submodule of __scikit-learn__ provides a function for one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with term frequencies, we lose information about the order of the words in the document.\n",
    "\n",
    "One-hot encoding as an extreme transformation: every term is equally important. This means terms that are relatively rare or unique still might be underemphasized (this is also a problem for term frequencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Term frequency-inverse document frequency_ (tf-idf) statistics put terms on approximately the same scale while also emphasizing relatively rare terms. There are [several different tf-idf statistics](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "The _smoothed tf-idf_, for a term $t$ and document $d$, is given by:\n",
    "\n",
    "$$\n",
    "\\operatorname{tf-idf}(t, d) = \\operatorname{tf}(t, d) \\cdot \\log \\left( \\frac{N}{1 + n_t} \\right)\n",
    "$$\n",
    "\n",
    "where $N$ is the total number of documents and $n_t$ is the number of documents that contain $t$.\n",
    "\n",
    "The `sklearn.feature_extraction.text` submodule of __scikit-learn__ provides a function for computing tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In long documents or documents with many high-frequency terms, we can further reduce the emphasis on these terms by taking the logarithm of the term frequency. To do this, set `sublinear_tf = True` in the `TfidfVectorizer()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bag-of-words Model\n",
    "\n",
    "The one-hot encoding, term frequencies, and TF-IDF scores all ignore word order.\n",
    "\n",
    "The _bag-of-words model_ assumes that the order of words in a document doesn't matter. Imagine taking the words in each document and dumping them into a bag, where they get all mixed up. Note that in this case \"model\" means a way of thinking about a problem, not a statistical model.\n",
    "\n",
    "While the order of words in a document might seem important, the bag-of-words model is surprisingly useful. The bag-of-words model is a good place to start if you want to use statistical methods on language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Similarity\n",
    "\n",
    "We can measure the _similarity_ of two documents by computing the distance between their term frequency vectors. There are many different ways we can measure distance:\n",
    "\n",
    "* Minkowski distance, a family of distances that includes Euclidean distance and Manhattan distance.\n",
    "* Mahalanobis distance, the Euclidean distance between z-scores.\n",
    "* Cosine distance, the cosine of the angle between two vectors. See [here](https://stats.stackexchange.com/a/235676/29695) for an explanation of how cosine distance is related to correlation.\n",
    "* And others...\n",
    "\n",
    "The cosine distance often works well for language data. The cosine distance between two vectors $a$ and $b$ is defined as:\n",
    "\n",
    "$$\n",
    "\\frac{a \\cdot b}{\\Vert a \\Vert \\Vert b \\Vert}\n",
    "$$\n",
    "\n",
    "where $\\Vert \\cdot \\Vert$ is the Euclidean norm.\n",
    "\n",
    "The `TfidfVectorizer()` function already divides the returned tf-idf vectors by their Euclidean norms, so we can compute cosine distance as a simple dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The n-gram Model\n",
    "\n",
    "Remember how the bag-of-words model ignores word order?\n",
    "\n",
    "We can extend the model to keep some order by taking sequences of words instead of individual words. Sequences of two or three words are called _bigrams_ and _trigrams_, respectively. A sequence of $n$ words is called an _n-gram_.\n",
    "\n",
    "__nltk__ provides functions to extract n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a separate n-gram was computed for each word in the original document. So for the bigrams in the example, we get every pair of words that appears in the document.\n",
    "\n",
    "The n-gram model assumes that nearby words have the strongest effect on the meaning of each word.\n",
    "\n",
    "We can use n-grams to identify phrases that are particularly common in a document. We can also use the n-gram model to engineer features, the same way we used the bag-of-words model. That is, we can compute frequencies, one-hot encodings, TF-IDFs, and other features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases\n",
    "\n",
    "A _database_ is a collection of data. There are several different models for how to organize data in a database; these are called _database models_. In this context, \"model\" refers to a design or mental model, not a statistical model.\n",
    "\n",
    "The _relational model_ organizes data as a collection of tables. Tables have rows (also called _tuples_ or _records_) and columns (also called _attributes_). Most tables have a _key_ column that is unique for each row and _relates_ the table to other tables. The relational model is the most popular database model by far, and the one we'll focus on in this course.\n",
    "\n",
    "There are also many different software programs for managing databases, called _database management systems_ (DBMS). Each DBMS usually has its own format for storing data on disk, independent of the database model. Some popular DBMSes are:\n",
    "\n",
    "* [SQLite](https://www.sqlite.org/)\n",
    "* [MySQL](https://www.mysql.com/)\n",
    "* [Microsoft SQL Server](https://www.microsoft.com/en-us/sql-server)\n",
    "* [PostgreSQL](https://www.postgresql.org/)\n",
    "\n",
    "Why use a database? There are several reasons:\n",
    "\n",
    "* Your data may already be in a database, so converting to another format is extra work.\n",
    "* Database operations are highly optimized, so they typically take less time and memory than an equivalent operation in Python.\n",
    "* Database operations can run on datasets that are too large to fit in memory. Doing this in Python requires special programming strategies.\n",
    "* Many DBMSes provide built-in version control, multi-user access, and security checks.\n",
    "* Databases can be updated in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Query Language\n",
    "\n",
    "_Structured query language_ (SQL) is a language designed for querying information in relational databases.\n",
    "\n",
    "A free SQL tutorial is available [here](https://www.w3schools.com/sql/).\n",
    "\n",
    "### Getting Connected\n",
    "\n",
    "There are several ways to connect to a database and run SQL queries from Python:\n",
    "\n",
    "* The built-in __sqlite3__ module, which only supports SQLite.\n",
    "* The __sqlalchemy__ package, a unified interface for a variety of different SQL database formats (more than just SQLite). See the [tutorial](http://docs.sqlalchemy.org/en/latest/core/tutorial.html) for more details.\n",
    "\n",
    "We'll use a SQLite database here, since SQLite is possibly [the most-used database engine in the world](https://sqlite.org/mostdeployed.html). SQLite's popularity is partly due to its reliability, easy setup, and broad range of features.\n",
    "\n",
    "Let's connect to the suppliers database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to a database, use the module's `connect()` function. This is similar to opening a file; you should close the database when you're done using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute a SQL query, use the connection's `.execute()` method. This returns a _cursor_, which is a pointer to the results in the database (imagine a finger pointing at the results).\n",
    "\n",
    "SQLite databases store metadata in a special table called `sqlite_master`. We can use `sqlite_master` to find out the names of the other tables in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the results from the database, use one of the cursor's fetch methods. The `.fetchall()` method returns all rows in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `sqlite3` will return rows as tuples. If you'd rather have the rows as dictionaries indexed by column name, set the `.row_factory` attribute on the database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the rows will behave like dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to close the database when you're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll generally use the `pd.read_sql()` function in __pandas__ to run our SQL queries. \n",
    "\n",
    "The function takes a SQL query and an open database connection as arguments, so you still need to connect to the database first with `sqlite3` or `sqlalchemy`. The result of the query is returned as a data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SELECT`\n",
    "\n",
    "The `SELECT` command selects rows from a table. Most of your SQL queries will start with `SELECT`. The syntax is:\n",
    "\n",
    "```sql\n",
    "SELECT col1, col2, ... FROM my_table;\n",
    "```\n",
    "\n",
    "Here `col1`, `col2`, and so on are column names and `my_table` is a table name. You can select all columns with an asterisk  `*`.\n",
    "\n",
    "SQL is not case-sensitive and ignores whitespace, but the convention is to write SQL keywords in uppercase and column/table names in lowercase. A semicolon `;` marks the end of a SQL query, but this is optional for many tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlite_master is a special table included in every SQLite database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `LIMIT`\n",
    "\n",
    "The `SELECT` command can be extended with many other keywords.\n",
    "\n",
    "The first of these is `LIMIT`, which limits the number of rows returned. `LIMIT` is the SQL equivalent of Pandas' `.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DISTINCT`\n",
    "\n",
    "The `DISTINCT` keyword limits rows to distinct results. `DISTINCT` is the SQL equivalent of Pandas' `.drop_duplicates()` method.\n",
    "\n",
    "Keep in mind that `DISTINCT` applies to all of the selected columns, not just one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ORDER BY`\n",
    "\n",
    "The `ORDER BY` keyword sorts the returned rows. `ORDER BY` is the SQL equivalent of Pandas' `.sort_values()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the suffix `ASC` for an ascending sort (smallest to largest) and `DESC` for a descending sort (largest to smallest).\n",
    "\n",
    "In SQLite, the default is ascending, but other other databases may differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `WHERE`\n",
    "\n",
    "`WHERE` puts conditions on the rows returned. `WHERE` is the SQL equivalent of subsetting.\n",
    "\n",
    "You can use `=` to test equality. Other comparison operators, such as `>=`, are also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `AND` and `OR` to combine conditions. You can also use parenthesis to indicate the order of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `IN` to check whether a value is in a collection of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL's `LIKE` keyword does simple pattern-matching language for strings. This is less powerful than regular expressions, but still useful.\n",
    "\n",
    "* `%` matches zero or more of any character, similar to regex `.*`\n",
    "* `_` matches any one character, similar to regex `.`\n",
    "\n",
    "In other databases (but not SQLite):\n",
    "* `[]` matches any one of the characters you put inside the brackects, identical to regex `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BETWEEN` keyword is useful for selecting ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operators\n",
    "\n",
    "You can use arithmetic operators `+`, `-`, `*`, `\\` on SQL columns to perform columnwise computations. These are the SQL equivalent of vectorized arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `AS`\n",
    "\n",
    "You can rename a column with the `AS` keyword. This keyword is especially useful together with SQL arithmetic operators and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
