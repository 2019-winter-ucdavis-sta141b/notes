{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 141B Lecture 10\n",
    "\n",
    "The class website is <https://github.com/2019-winter-ucdavis-sta141b/notes>\n",
    "\n",
    "### Announcements\n",
    "\n",
    "### Topics\n",
    "\n",
    "* Web Scraping\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* [CUESA's Vegetable Seasons Chart](https://cuesa.org/eat-seasonally/charts/vegetables)\n",
    "* [Craigslist Apartments](https://sacramento.craigslist.org/d/apts-housing-for-rent/search/apa)\n",
    "\n",
    "### References\n",
    "\n",
    "* [__requests__ documentation](http://docs.python-requests.org/en/master/)\n",
    "* [__requests-html__ documentation](https://html.python-requests.org/)\n",
    "* [MDN HTML Reference](https://developer.mozilla.org/en-US/docs/Web/HTML/Element)\n",
    "* [XPath Diner](http://www.topswagcode.com/xpath/) -- an interactive XPath tutorial\n",
    "* [CSS Diner](https://flukeout.github.io/) -- an interactive CSS Selector tutorial\n",
    "* Python for Data Analysis, Ch. 6\n",
    "* Python for Data Analysis, Ch. 7.3 (to review string processing)\n",
    "\n",
    "[PDSH]: https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "[ProGit]: https://git-scm.com/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our usual data science tools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Web scraping tools\n",
    "import lxml.html as lx\n",
    "import requests\n",
    "import requests_cache\n",
    "\n",
    "requests_cache.install_cache(\"../mycache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS Selectors\n",
    "\n",
    "To use CSS selectors in __lxml__, you also need to install the __cssselect__ package. To install for Anaconda, run\n",
    "\n",
    "```shell\n",
    "conda install -c anaconda cssselect\n",
    "```\n",
    "\n",
    "in an Anaconda Prompt (Windows) or Terminal (MacOS).\n",
    "\n",
    "If you don't use Anaconda, the package can be installed with `pip`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: CUESA Vegetable Seasons\n",
    "\n",
    "CUESA (Center for Urban Education about Sustainable Agriculture) provides [a chart](https://cuesa.org/eat-seasonally/charts/vegetables) that shows when vegetables are in season. Let's scrape the chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the page\n",
    "response = requests.get(\"https://cuesa.org/eat-seasonally/charts/vegetables\")\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the HTML\n",
    "html = lx.fromstring(response.text)\n",
    "html\n",
    "\n",
    "# Find the table with the vegetable seasons\n",
    "tab = html.xpath(\"//table\")[0]\n",
    "\n",
    "rows = tab.xpath(\".//tr\")\n",
    "rows = rows[1:]\n",
    "\n",
    "# Extract the header\n",
    "header = rows[0]\n",
    "\n",
    "header = [x.text for x in header.xpath(\".//li\")]\n",
    "\n",
    "# To be continued in the next lecture..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we generalize our scraper to the [chart](https://cuesa.org/eat-seasonally/charts/fruit) for fruit and nuts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Craigslist Apartments\n",
    "\n",
    "[Craigslist](https://www.craigslist.org/) is a popular website where people can post advertisements for free. We can use data from Craigslist to analyze the local rental market for apartments.\n",
    "\n",
    "Craigslist doesn't provide an API, so we have to scrape the data ourselves. Scraping Craigslist is the biggest challenge we've faced yet, since each ad is on a separate page.\n",
    "\n",
    "We can start by scraping the front page of the [apartments section](https://sacramento.craigslist.org/d/apts-housing-for-rent/search/apa) for links to individual ads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
