{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 141B Lecture 9\n",
    "\n",
    "The class website is <https://github.com/2019-winter-ucdavis-sta141b/notes>\n",
    "\n",
    "### Announcements\n",
    "\n",
    "### Topics\n",
    "\n",
    "* Undocumented APIs\n",
    "* XML and HTML\n",
    "* Web Scraping\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* [Yolo County Health Inspections](https://yoloeco.envisionconnect.com/)\n",
    "* [Wikipedia's List of Largest Cities](https://en.wikipedia.org/wiki/List_of_largest_cities)\n",
    "* [CUESA's Vegetable Seasons Chart](https://cuesa.org/eat-seasonally/charts/vegetables)\n",
    "\n",
    "### References\n",
    "\n",
    "* [__requests__ documentation](http://docs.python-requests.org/en/master/)\n",
    "* [__requests-html__ documentation](https://html.python-requests.org/)\n",
    "* [MDN HTML Reference](https://developer.mozilla.org/en-US/docs/Web/HTML/Element)\n",
    "* [XPath Diner](http://www.topswagcode.com/xpath/) -- an interactive XPath tutorial\n",
    "* [CSS Diner](https://flukeout.github.io/) -- an interactive CSS Selector tutorial\n",
    "* Python for Data Analysis, Ch. 6\n",
    "* Python for Data Analysis, Ch. 7.3 (to review string processing)\n",
    "\n",
    "[PDSH]: https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "[ProGit]: https://git-scm.com/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data from the Web\n",
    "\n",
    "Revised list of ways you can get data from the web, from most to least convenient:\n",
    "\n",
    "1. Direct download or \"data dump\"\n",
    "2. Python or R package (there are packages for many popular web APIs)\n",
    "3. Documented web API\n",
    "4. Undocumented web API\n",
    "5. Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undocumented Web APIs\n",
    "\n",
    "Many websites use undocumented web APIs to get data. For example:\n",
    "\n",
    "* [University of California Compensation](https://ucannualwage.ucop.edu/wage/)\n",
    "* [Yolo County Health Inspections](https://yoloeco.envisionconnect.com/)\n",
    "\n",
    "You can identify these websites by looking at requests in your browser's developer tools. In Firefox or Chrome, you can open the developer tools with `ctrl-shift-i`.\n",
    "\n",
    "Requests to web APIs almost always return JSON or XML data. By examining the browser requests, you can work out the endpoints and parameters, allowing you to use the API.\n",
    "\n",
    "**CAUTION:** Web APIs that are undocumented are often undocumented for a reason. Using an undocumented API may make someone angry or get you into legal trouble! Government and quasi-government websites (like the examples above) are probably okay, as long as you cache and rate-limit your requests. For everything else, find for an alternative or get permission first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reverse engineer the Yolo County Health Inspections web API so that we can get data about local restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "\n",
    "requests_cache.install_cache(\"mycache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could reverse engineer other parts of the API to get detailed data about health violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes a web page?\n",
    "\n",
    "Web pages are written in _hypertext markup language_ (HTML). HTML files (`.htm` or `.html`) are plain text, just like JSON, Python scripts, and R scripts.\n",
    "\n",
    "In HTML, we use _tags_ to create _elements_ of a web page. Elements add formatting and structure to the page.\n",
    "\n",
    "* Tags usually come in pairs: an opening tag and a closing tag.\n",
    "* Tags are written `<NAME>` for opening tags, `</NAME>` for closing tags, and `<NAME />` for singleton tags.\n",
    "* Opening and singleton tags can have _attributes_ that contain additional information. Attributes are written `ATTRIBUTE=VALUE` after the tag name. \n",
    "\n",
    "See [here](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/HTML_basics) for a more detailed explanation, and [here](https://developer.mozilla.org/en-US/docs/Web/HTML/Element) for a list of valid HTML elements.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "As an example:\n",
    "\n",
    "```html\n",
    "<p>This <a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\">page</a> is famous and this <strong>word</strong> is emphasized.</p>\n",
    "```\n",
    "The `p` tag marks a paragraph, the `a` tag marks a link (an _anchor_), and the `strong` tag marks emphasized text.\n",
    "\n",
    "Here's a string that contains HTML for a simple, complete website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <title>This is the Title!</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <p>This is a paragraph!</p>\n",
    "    <p id=\"best-paragraph\">This is another paragraph! ðŸŒ®</p>\n",
    "    <p>Visit <a href=\"https://pudding.cool/\">The Pudding</a>.</p>\n",
    "    <span>This is a span. It comes with an avocado. ðŸ¥‘</span>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Extensible markup language_ (XML) also uses tags to create elements. We say XML is _extensible_ because you can create your own XML elements (unlike HTML). People typically use XML to describe structure and meaning of data, rather than for formatting.\n",
    "\n",
    "We'll use the same process to extract data from both HTML and XML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Packages\n",
    "\n",
    "A _parser_ converts formatted data into familiar data structures. We've used __requests__' built-in JSON parser, but the package doesn't have a built-in HTML/XML parser. Fortunately, there are many other Python packages for parsing HTML/XML and web scraping.\n",
    "\n",
    "HTML/XML Parsers:\n",
    "* [lxml](https://lxml.de/)\n",
    "* [html5lib](https://github.com/html5lib/html5lib-python)\n",
    "* [beautifulsoup](https://www.crummy.com/software/BeautifulSoup/)\n",
    "* [requests-html](https://html.python-requests.org/)\n",
    "\n",
    "Scraper Frameworks (_convenient after learning the basics with parsers_):\n",
    "* [scrapy](https://scrapy.org/)\n",
    "* [newspaper3k](https://github.com/codelucas/newspaper)\n",
    "\n",
    "Even more [here](https://github.com/lorien/awesome-web-scraping/blob/master/python.md#web-scraping-frameworks).\n",
    "\n",
    "We'll use __lxml__ here, but you're welcome to use other packages on assignments and the project. To install __lxml__ for Anaconda, run `conda install -c anaconda lxml` in a shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html as lx\n",
    "\n",
    "html = lx.fromstring(page)\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Elements\n",
    "\n",
    "Elements are nested, so an HTML document is like a tree:\n",
    "```\n",
    "html\n",
    "â”œâ”€â”€ head\n",
    "â”‚   â””â”€â”€ title\n",
    "â””â”€â”€ body\n",
    "    â”œâ”€â”€ p\n",
    "    â”œâ”€â”€ p\n",
    "    â”œâ”€â”€ p\n",
    "    â”‚   â””â”€â”€ a\n",
    "    â””â”€â”€ span\n",
    "```\n",
    "This is similar to the file system on your computer. The key difference is that elements at the same level can have the same tag name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XPath\n",
    "\n",
    "The _XML Path Language_ (XPath) lets us write paths to elements. XPath paths look a lot like file paths. XPath is not Python-specific!\n",
    "\n",
    "The `.xpath()` method gets all elements at an XPath path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there may be more than one element, the method always returns a list.\n",
    "\n",
    "Absolute paths are not robust for scraping. An update to a web page that adds a single tag can break a scraper that uses absolute paths. In XPath, `//` means \"anywhere below\". We'll use `//` often because it's more robust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we just elements that satisfy a certain condition? In XPath, `[ ]` filters out elements that don't match a condition. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[XPath Diner](http://www.topswagcode.com/xpath/) is an interactive tutorial that teaches most of the XPath syntax. It takes about 20-60 minutes. Work through it to become an XPath ninja!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSS Selectors\n",
    "\n",
    "_Cascading Style Sheets_ (CSS) is another language for formatting elements in an HTML document. CSS provides another way to select elements, called _CSS selectors_.\n",
    "\n",
    "CSS selectors are more concise but less flexible than XPath paths. The `.cssselect()` method gets all elements at a CSS selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Text and Attributes\n",
    "\n",
    "There are two ways to get text from an element:\n",
    "\n",
    "* `.text` gives text inside the element, but not its children\n",
    "* `.text_content()` gives text inside the element and its children, with all tags removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get values from attributes on an element with `.attrib`, which is a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Scraping Tables\n",
    "\n",
    "For data in a `table` element, we can use __Pandas__ instead of writing a scraper.\n",
    "\n",
    "Wikipedia provides lots of useful information in tables. Let's get the Wikipedia list of [the world's largest cities][wiki].\n",
    "\n",
    "[wiki]: https://en.wikipedia.org/wiki/List_of_largest_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Scrapers\n",
    "\n",
    "When the data we want isn't in a `table` element, we have to write our own scraper.\n",
    "\n",
    "The workflow for writing a scraper is the same regardless of the language you use:\n",
    "\n",
    "1. Download pages with an HTTP request (usually `GET`)\n",
    "2. Parse pages to extract text\n",
    "3. Clean up extracted text with string methods or regex\n",
    "4. Save cleaned results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: CUESA Vegetable Seasons\n",
    "\n",
    "\n",
    "\n",
    "CUESA (Center for Urban Education about Sustainable Agriculture) provides [a chart](https://cuesa.org/eat-seasonally/charts/vegetables) that shows when vegetables are in season. Let's scrape the chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we generalize our scraper to the [chart](https://cuesa.org/eat-seasonally/charts/fruit) for fruit and nuts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
